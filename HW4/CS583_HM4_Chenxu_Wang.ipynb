{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQtZK6BSuCrW"
   },
   "source": [
    "# Home 4: Build a CNN for image recognition.\n",
    "\n",
    "### Name: [Chenxu Wang]\n",
    "### CWID: [10457625]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aC3U9A8fuCrb"
   },
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read, complete, and run the code.\n",
    "\n",
    "2. **Make substantial improvements** to maximize the accurcy.\n",
    "    \n",
    "3. Convert the .IPYNB file to .HTML file.\n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "    \n",
    "    * Missing **the output after execution** will not be graded.\n",
    "    \n",
    "4. Upload this .HTML file to your Google Drive, Dropbox, or Github repo. (If you submit the file to Google Drive or Dropbox, you must make the file \"open-access\". The delay caused by \"deny of access\" may result in late penalty.)\n",
    "\n",
    "4. Submit the link to this .HTML file to Canvas.\n",
    "\n",
    "    * Example: https://github.com/wangshusen/CS583-2020S/blob/master/homework/HM4/HM4.html\n",
    "\n",
    "\n",
    "## Requirements:\n",
    "\n",
    "1. You can use whatever CNN architecture, including VGG, Inception, and ResNet. However, you must build the networks layer by layer. You must NOT import the archetectures from ```keras.applications```.\n",
    "\n",
    "2. Make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer.\n",
    "\n",
    "3. If you want to regularize a ```Conv```/```Dense``` layer, you should place a ```Dropout``` layer **before** the ```Conv```/```Dense``` layer.\n",
    "\n",
    "4. An accuracy above 70% is considered reasonable. An accuracy above 80% is considered good. Without data augmentation, achieving 80% accuracy is difficult.\n",
    "\n",
    "\n",
    "## Google Colab\n",
    "\n",
    "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option.\n",
    "\n",
    "- Keep in mind that you must download it as an IPYNB file and then use IPython Notebook to convert it to HTML.\n",
    "\n",
    "- Also keep in mind that the IPYNB and HTML files must contain the outputs. (Otherwise, the instructor will not be able to know the correctness and performance.) Do the followings to keep the outputs.\n",
    "\n",
    "- In Colab, go to ```Runtime``` --> ```Change runtime type``` --> Do NOT check ```Omit code cell output when saving this notebook```. In this way, the downloaded IPYNB file contains the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VW4BaBU_uCrc"
   },
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ShweauyuCrd"
   },
   "source": [
    "### 1.1. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o091nWojuCre",
    "outputId": "3be37ad2-3a40-48ea-9c14-d8e2c5b79575"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (50000, 32, 32, 3)\n",
      "shape of y_train: (50000, 1)\n",
      "shape of x_test: (10000, 32, 32, 3)\n",
      "shape of y_test: (10000, 1)\n",
      "number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('shape of x_train: ' + str(x_train.shape))\n",
    "print('shape of y_train: ' + str(y_train.shape))\n",
    "print('shape of x_test: ' + str(x_test.shape))\n",
    "print('shape of y_test: ' + str(y_test.shape))\n",
    "print('number of classes: ' + str(np.max(y_train) - np.min(y_train) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkitFJphz5Yu"
   },
   "outputs": [],
   "source": [
    "# print(x_train[5])\n",
    "#x_train = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaurkEEDuCrf"
   },
   "source": [
    "### 1.2. One-hot encode the labels\n",
    "\n",
    "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
    "\n",
    "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
    "\n",
    "2. Apply the function to ```y_train``` and ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "weWNzj1ruCrf",
    "outputId": "6d30485d-dd70-4ba2-9c69-8b2f1caabba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_vec: (50000, 10)\n",
      "Shape of y_test_vec: (10000, 10)\n",
      "[6]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(y, num_class=10):\n",
    "  onehot_encoded = np.zeros((y.size, num_class))\n",
    "  onehot_encoded[np.arange(y.size), y.reshape((1, y.size))] = 1\n",
    "  return onehot_encoded\n",
    "def to_norm_data(train_x, test_x):\n",
    "  norm_train_x = train_x.astype('float32') / 255\n",
    "  norm_test_x = test_x.astype('float32') / 255\n",
    "  return norm_train_x, norm_test_x\n",
    "\n",
    "\n",
    "x_train, x_test = to_norm_data(x_train, x_test)\n",
    "\n",
    "y_train_vec = to_one_hot(y_train)\n",
    "y_test_vec = to_one_hot(y_test)\n",
    "\n",
    "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
    "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STEDr4e1uCrf"
   },
   "source": [
    "#### Remark: the outputs should be\n",
    "* Shape of y_train_vec: (50000, 10)\n",
    "* Shape of y_test_vec: (10000, 10)\n",
    "* [6]\n",
    "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RdeBO6JuCrg"
   },
   "source": [
    "### 1.3. Randomly partition the training set to training and validation sets\n",
    "\n",
    "Randomly partition the 50K training samples to 2 sets:\n",
    "* a training set containing 40K samples\n",
    "* a validation set containing 10K samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_0wZVUmuCrh",
    "outputId": "eaad7fca-db5a-4817-defa-244fc656bf53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_tr: (40000, 32, 32, 3)\n",
      "Shape of y_tr: (40000, 10)\n",
      "Shape of x_val: (10000, 32, 32, 3)\n",
      "Shape of y_val: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "rand_indices = np.random.permutation(50000)\n",
    "train_indices = rand_indices[0:40000]\n",
    "valid_indices = rand_indices[40000:50000]\n",
    "\n",
    "x_val = x_train[valid_indices, :]\n",
    "y_val = y_train_vec[valid_indices, :]\n",
    "\n",
    "x_tr = x_train[train_indices, :]\n",
    "y_tr = y_train_vec[train_indices, :]\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_tr.shape))\n",
    "print('Shape of y_tr: ' + str(y_tr.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtLQwOs_uCrh"
   },
   "source": [
    "## 2. Build a CNN and tune its hyper-parameters\n",
    "\n",
    "1. Build a convolutional neural network model\n",
    "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
    "    * Do NOT use test data for hyper-parameter tuning!!!\n",
    "3. Try to achieve a validation accuracy as high as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grVDzPoduCri"
   },
   "source": [
    "### Remark: \n",
    "\n",
    "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
    "* Add more layers.\n",
    "* Use regularizations, e.g., dropout.\n",
    "* Use batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o7c7WOkSuCri",
    "outputId": "81a7b27c-221d-4bd6-c8ba-33bd5b68648b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 552,874\n",
      "Trainable params: 551,722\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sSz4MmatuCri"
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    " # to be tuned!\n",
    "learning_rate = 1E-3\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ujs0xMAuCri",
    "outputId": "98ebfe1f-bdff-4c8d-832b-11424924973b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 11s 7ms/step - loss: 1.8740 - acc: 0.3501 - val_loss: 1.3035 - val_acc: 0.5715\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 1.1526 - acc: 0.6000 - val_loss: 1.0902 - val_acc: 0.6312\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.9834 - acc: 0.6587 - val_loss: 0.9826 - val_acc: 0.6919\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.9178 - acc: 0.6868 - val_loss: 0.8035 - val_acc: 0.7384\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.8525 - acc: 0.7170 - val_loss: 1.0516 - val_acc: 0.6936\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.8123 - acc: 0.7301 - val_loss: 0.8074 - val_acc: 0.7367\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.7747 - acc: 0.7459 - val_loss: 0.7462 - val_acc: 0.7556\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.7651 - acc: 0.7517 - val_loss: 0.7867 - val_acc: 0.7745\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.7289 - acc: 0.7628 - val_loss: 0.7926 - val_acc: 0.7705\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.7234 - acc: 0.7682 - val_loss: 1.0709 - val_acc: 0.7426\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.7017 - acc: 0.7721 - val_loss: 0.8802 - val_acc: 0.7696\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.6949 - acc: 0.7780 - val_loss: 0.6581 - val_acc: 0.7829\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.6677 - acc: 0.7857 - val_loss: 0.6843 - val_acc: 0.7900\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.6596 - acc: 0.7867 - val_loss: 0.7494 - val_acc: 0.7878\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.6541 - acc: 0.7882 - val_loss: 0.6324 - val_acc: 0.7953\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.6360 - acc: 0.7982 - val_loss: 0.6809 - val_acc: 0.7876\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.6256 - acc: 0.7983 - val_loss: 0.7311 - val_acc: 0.7579\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.6122 - acc: 0.7995 - val_loss: 0.6523 - val_acc: 0.7923\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.5941 - acc: 0.8089 - val_loss: 0.6750 - val_acc: 0.8093\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.5865 - acc: 0.8114 - val_loss: 0.6243 - val_acc: 0.8072\n",
      "Epoch 21/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.5704 - acc: 0.8135 - val_loss: 0.6050 - val_acc: 0.8064\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.5747 - acc: 0.8164 - val_loss: 0.5514 - val_acc: 0.8263\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.5591 - acc: 0.8224 - val_loss: 0.5799 - val_acc: 0.8138\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.5461 - acc: 0.8231 - val_loss: 0.6102 - val_acc: 0.8094\n",
      "Epoch 25/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.5549 - acc: 0.8199 - val_loss: 0.6135 - val_acc: 0.8121\n",
      "Epoch 26/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.5352 - acc: 0.8251 - val_loss: 0.5990 - val_acc: 0.8145\n",
      "Epoch 27/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.5309 - acc: 0.8285 - val_loss: 0.6259 - val_acc: 0.8151\n",
      "Epoch 28/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.5230 - acc: 0.8311 - val_loss: 0.5760 - val_acc: 0.8160\n",
      "Epoch 29/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.5211 - acc: 0.8317 - val_loss: 0.5304 - val_acc: 0.8331\n",
      "Epoch 30/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.5127 - acc: 0.8300 - val_loss: 0.5858 - val_acc: 0.8185\n",
      "Epoch 31/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4960 - acc: 0.8410 - val_loss: 0.5788 - val_acc: 0.8250\n",
      "Epoch 32/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4947 - acc: 0.8396 - val_loss: 0.5568 - val_acc: 0.8309\n",
      "Epoch 33/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.5026 - acc: 0.8381 - val_loss: 0.5721 - val_acc: 0.8120\n",
      "Epoch 34/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4910 - acc: 0.8373 - val_loss: 0.5650 - val_acc: 0.8243\n",
      "Epoch 35/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4722 - acc: 0.8487 - val_loss: 0.5424 - val_acc: 0.8176\n",
      "Epoch 36/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4678 - acc: 0.8499 - val_loss: 0.6342 - val_acc: 0.8212\n",
      "Epoch 37/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4706 - acc: 0.8481 - val_loss: 0.5693 - val_acc: 0.8236\n",
      "Epoch 38/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4609 - acc: 0.8482 - val_loss: 0.5562 - val_acc: 0.8139\n",
      "Epoch 39/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4446 - acc: 0.8543 - val_loss: 0.6186 - val_acc: 0.8125\n",
      "Epoch 40/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4556 - acc: 0.8514 - val_loss: 0.5219 - val_acc: 0.8341\n",
      "Epoch 41/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4465 - acc: 0.8520 - val_loss: 0.5312 - val_acc: 0.8360\n",
      "Epoch 42/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4430 - acc: 0.8550 - val_loss: 0.5102 - val_acc: 0.8416\n",
      "Epoch 43/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4470 - acc: 0.8565 - val_loss: 0.5353 - val_acc: 0.8317\n",
      "Epoch 44/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4308 - acc: 0.8608 - val_loss: 0.4944 - val_acc: 0.8407\n",
      "Epoch 45/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4274 - acc: 0.8609 - val_loss: 0.4741 - val_acc: 0.8475\n",
      "Epoch 46/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4267 - acc: 0.8638 - val_loss: 0.5305 - val_acc: 0.8357\n",
      "Epoch 47/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4216 - acc: 0.8605 - val_loss: 0.4940 - val_acc: 0.8531\n",
      "Epoch 48/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4117 - acc: 0.8661 - val_loss: 0.5641 - val_acc: 0.8508\n",
      "Epoch 49/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4352 - acc: 0.8611 - val_loss: 0.5415 - val_acc: 0.8335\n",
      "Epoch 50/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4184 - acc: 0.8644 - val_loss: 0.5123 - val_acc: 0.8389\n",
      "Epoch 51/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4175 - acc: 0.8659 - val_loss: 0.5422 - val_acc: 0.8299\n",
      "Epoch 52/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.4049 - acc: 0.8696 - val_loss: 0.4885 - val_acc: 0.8471\n",
      "Epoch 53/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3951 - acc: 0.8741 - val_loss: 0.5381 - val_acc: 0.8323\n",
      "Epoch 54/100\n",
      "1250/1250 [==============================] - 9s 8ms/step - loss: 0.4118 - acc: 0.8659 - val_loss: 0.5295 - val_acc: 0.8366\n",
      "Epoch 55/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3980 - acc: 0.8679 - val_loss: 0.5163 - val_acc: 0.8405\n",
      "Epoch 56/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3973 - acc: 0.8708 - val_loss: 0.4847 - val_acc: 0.8434\n",
      "Epoch 57/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3921 - acc: 0.8725 - val_loss: 0.5463 - val_acc: 0.8284\n",
      "Epoch 58/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3935 - acc: 0.8716 - val_loss: 0.5260 - val_acc: 0.8344\n",
      "Epoch 59/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3872 - acc: 0.8737 - val_loss: 0.4947 - val_acc: 0.8513\n",
      "Epoch 60/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3866 - acc: 0.8769 - val_loss: 0.5798 - val_acc: 0.8162\n",
      "Epoch 61/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3830 - acc: 0.8776 - val_loss: 0.5600 - val_acc: 0.8409\n",
      "Epoch 62/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3724 - acc: 0.8806 - val_loss: 0.4548 - val_acc: 0.8558\n",
      "Epoch 63/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3862 - acc: 0.8728 - val_loss: 0.4881 - val_acc: 0.8555\n",
      "Epoch 64/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3743 - acc: 0.8794 - val_loss: 0.5119 - val_acc: 0.8399\n",
      "Epoch 65/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3737 - acc: 0.8801 - val_loss: 0.4596 - val_acc: 0.8538\n",
      "Epoch 66/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3755 - acc: 0.8808 - val_loss: 0.4920 - val_acc: 0.8496\n",
      "Epoch 67/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3731 - acc: 0.8813 - val_loss: 0.5170 - val_acc: 0.8419\n",
      "Epoch 68/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3712 - acc: 0.8807 - val_loss: 0.5240 - val_acc: 0.8460\n",
      "Epoch 69/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3657 - acc: 0.8820 - val_loss: 0.5120 - val_acc: 0.8563\n",
      "Epoch 70/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3680 - acc: 0.8822 - val_loss: 0.4676 - val_acc: 0.8516\n",
      "Epoch 71/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3735 - acc: 0.8810 - val_loss: 0.4963 - val_acc: 0.8488\n",
      "Epoch 72/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3639 - acc: 0.8812 - val_loss: 0.4903 - val_acc: 0.8467\n",
      "Epoch 73/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3609 - acc: 0.8851 - val_loss: 0.4969 - val_acc: 0.8584\n",
      "Epoch 74/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3660 - acc: 0.8838 - val_loss: 0.4570 - val_acc: 0.8598\n",
      "Epoch 75/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3549 - acc: 0.8842 - val_loss: 0.5548 - val_acc: 0.8385\n",
      "Epoch 76/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3461 - acc: 0.8875 - val_loss: 0.6143 - val_acc: 0.8093\n",
      "Epoch 77/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3524 - acc: 0.8868 - val_loss: 0.4587 - val_acc: 0.8574\n",
      "Epoch 78/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3569 - acc: 0.8858 - val_loss: 0.4770 - val_acc: 0.8507\n",
      "Epoch 79/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3426 - acc: 0.8880 - val_loss: 0.4870 - val_acc: 0.8481\n",
      "Epoch 80/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3485 - acc: 0.8858 - val_loss: 0.5277 - val_acc: 0.8525\n",
      "Epoch 81/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3500 - acc: 0.8855 - val_loss: 0.4529 - val_acc: 0.8581\n",
      "Epoch 82/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3342 - acc: 0.8886 - val_loss: 0.4994 - val_acc: 0.8447\n",
      "Epoch 83/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3315 - acc: 0.8926 - val_loss: 0.4920 - val_acc: 0.8610\n",
      "Epoch 84/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3372 - acc: 0.8910 - val_loss: 0.4770 - val_acc: 0.8473\n",
      "Epoch 85/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3516 - acc: 0.8842 - val_loss: 0.4513 - val_acc: 0.8563\n",
      "Epoch 86/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3358 - acc: 0.8918 - val_loss: 0.5140 - val_acc: 0.8496\n",
      "Epoch 87/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3399 - acc: 0.8902 - val_loss: 0.5088 - val_acc: 0.8509\n",
      "Epoch 88/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3411 - acc: 0.8872 - val_loss: 0.5362 - val_acc: 0.8388\n",
      "Epoch 89/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3405 - acc: 0.8921 - val_loss: 0.4598 - val_acc: 0.8575\n",
      "Epoch 90/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3355 - acc: 0.8934 - val_loss: 0.5346 - val_acc: 0.8415\n",
      "Epoch 91/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3354 - acc: 0.8922 - val_loss: 0.5192 - val_acc: 0.8341\n",
      "Epoch 92/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3355 - acc: 0.8959 - val_loss: 0.4793 - val_acc: 0.8588\n",
      "Epoch 93/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3357 - acc: 0.8902 - val_loss: 0.4851 - val_acc: 0.8604\n",
      "Epoch 94/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3292 - acc: 0.8947 - val_loss: 0.4957 - val_acc: 0.8540\n",
      "Epoch 95/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3319 - acc: 0.8923 - val_loss: 0.5773 - val_acc: 0.8392\n",
      "Epoch 96/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3319 - acc: 0.8946 - val_loss: 0.4657 - val_acc: 0.8615\n",
      "Epoch 97/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3310 - acc: 0.8926 - val_loss: 0.5428 - val_acc: 0.8455\n",
      "Epoch 98/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3285 - acc: 0.8938 - val_loss: 0.4897 - val_acc: 0.8564\n",
      "Epoch 99/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3295 - acc: 0.8961 - val_loss: 0.5026 - val_acc: 0.8463\n",
      "Epoch 100/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.3288 - acc: 0.8960 - val_loss: 0.4540 - val_acc: 0.8628\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_tr, y_tr, batch_size=32, epochs=100, validation_data=(x_val, y_val))\n",
    "model.save('non_data_augmt_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "S29yHJY1JO6U",
    "outputId": "60408b2b-85c3-4900-8e5b-272c59ef2b1b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MkL8ynQiJe7"
   },
   "outputs": [],
   "source": [
    "# using data augmentation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-5X5OOQij7p",
    "outputId": "6e1ed588-ed44-45be-8f56-ff23cb5946c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1250/1250 [==============================] - 27s 20ms/step - loss: 2.0439 - acc: 0.2864 - val_loss: 1.4714 - val_acc: 0.5222\n",
      "Epoch 2/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.4891 - acc: 0.4654 - val_loss: 1.3245 - val_acc: 0.5595\n",
      "Epoch 3/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.3700 - acc: 0.5138 - val_loss: 1.7179 - val_acc: 0.5070\n",
      "Epoch 4/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.2876 - acc: 0.5487 - val_loss: 1.3217 - val_acc: 0.5819\n",
      "Epoch 5/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.2285 - acc: 0.5752 - val_loss: 1.3301 - val_acc: 0.6065\n",
      "Epoch 6/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.1947 - acc: 0.5937 - val_loss: 1.3843 - val_acc: 0.6047\n",
      "Epoch 7/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.1638 - acc: 0.6047 - val_loss: 1.5321 - val_acc: 0.6287\n",
      "Epoch 8/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.1356 - acc: 0.6172 - val_loss: 1.1464 - val_acc: 0.6462\n",
      "Epoch 9/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.1276 - acc: 0.6213 - val_loss: 1.1791 - val_acc: 0.6463\n",
      "Epoch 10/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.0973 - acc: 0.6320 - val_loss: 1.8069 - val_acc: 0.5550\n",
      "Epoch 11/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.0863 - acc: 0.6384 - val_loss: 1.2013 - val_acc: 0.6787\n",
      "Epoch 12/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.0638 - acc: 0.6453 - val_loss: 1.0485 - val_acc: 0.6697\n",
      "Epoch 13/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.0490 - acc: 0.6530 - val_loss: 1.3145 - val_acc: 0.6346\n",
      "Epoch 14/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.0223 - acc: 0.6617 - val_loss: 1.0873 - val_acc: 0.6689\n",
      "Epoch 15/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.0137 - acc: 0.6626 - val_loss: 0.8690 - val_acc: 0.7015\n",
      "Epoch 16/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.0181 - acc: 0.6647 - val_loss: 1.1373 - val_acc: 0.6706\n",
      "Epoch 17/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.0052 - acc: 0.6683 - val_loss: 0.8769 - val_acc: 0.7191\n",
      "Epoch 18/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.9731 - acc: 0.6775 - val_loss: 0.9045 - val_acc: 0.7280\n",
      "Epoch 19/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.9819 - acc: 0.6773 - val_loss: 0.7819 - val_acc: 0.7477\n",
      "Epoch 20/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.9587 - acc: 0.6794 - val_loss: 0.8646 - val_acc: 0.7267\n",
      "Epoch 21/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.9363 - acc: 0.6899 - val_loss: 0.9113 - val_acc: 0.7137\n",
      "Epoch 22/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.9384 - acc: 0.6915 - val_loss: 0.9031 - val_acc: 0.7265\n",
      "Epoch 23/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.9354 - acc: 0.6922 - val_loss: 0.7539 - val_acc: 0.7551\n",
      "Epoch 24/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.9293 - acc: 0.6938 - val_loss: 1.1717 - val_acc: 0.6717\n",
      "Epoch 25/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.9224 - acc: 0.6964 - val_loss: 1.0065 - val_acc: 0.7008\n",
      "Epoch 26/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.8934 - acc: 0.7080 - val_loss: 1.0732 - val_acc: 0.6672\n",
      "Epoch 27/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.9048 - acc: 0.6993 - val_loss: 0.7516 - val_acc: 0.7587\n",
      "Epoch 28/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.8781 - acc: 0.7088 - val_loss: 0.8352 - val_acc: 0.7334\n",
      "Epoch 29/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.8694 - acc: 0.7110 - val_loss: 0.8114 - val_acc: 0.7530\n",
      "Epoch 30/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.8709 - acc: 0.7123 - val_loss: 0.6710 - val_acc: 0.7796\n",
      "Epoch 31/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.8664 - acc: 0.7124 - val_loss: 0.6874 - val_acc: 0.7806\n",
      "Epoch 32/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.8522 - acc: 0.7195 - val_loss: 1.0089 - val_acc: 0.7020\n",
      "Epoch 33/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.8536 - acc: 0.7218 - val_loss: 1.1860 - val_acc: 0.7015\n",
      "Epoch 34/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.8531 - acc: 0.7167 - val_loss: 0.6737 - val_acc: 0.7772\n",
      "Epoch 35/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.8445 - acc: 0.7209 - val_loss: 0.5873 - val_acc: 0.8089\n",
      "Epoch 36/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.8259 - acc: 0.7315 - val_loss: 0.8278 - val_acc: 0.7436\n",
      "Epoch 37/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.8239 - acc: 0.7298 - val_loss: 0.7775 - val_acc: 0.7574\n",
      "Epoch 38/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.8249 - acc: 0.7278 - val_loss: 0.7932 - val_acc: 0.7603\n",
      "Epoch 39/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.8203 - acc: 0.7295 - val_loss: 0.6261 - val_acc: 0.8042\n",
      "Epoch 40/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.8109 - acc: 0.7352 - val_loss: 1.1205 - val_acc: 0.7280\n",
      "Epoch 41/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.8215 - acc: 0.7357 - val_loss: 0.6839 - val_acc: 0.7856\n",
      "Epoch 42/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.8128 - acc: 0.7306 - val_loss: 0.6713 - val_acc: 0.7892\n",
      "Epoch 43/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.8005 - acc: 0.7338 - val_loss: 0.7017 - val_acc: 0.7843\n",
      "Epoch 44/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.7952 - acc: 0.7413 - val_loss: 0.6279 - val_acc: 0.7984\n",
      "Epoch 45/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.8049 - acc: 0.7361 - val_loss: 0.5864 - val_acc: 0.8101\n",
      "Epoch 46/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.7959 - acc: 0.7406 - val_loss: 0.6192 - val_acc: 0.8023\n",
      "Epoch 47/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.7906 - acc: 0.7443 - val_loss: 0.6282 - val_acc: 0.7948\n",
      "Epoch 48/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.7872 - acc: 0.7413 - val_loss: 0.7222 - val_acc: 0.7762\n",
      "Epoch 49/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.7786 - acc: 0.7469 - val_loss: 0.8072 - val_acc: 0.7575\n",
      "Epoch 50/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.7879 - acc: 0.7414 - val_loss: 0.6690 - val_acc: 0.7964\n",
      "Epoch 51/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.7866 - acc: 0.7466 - val_loss: 0.6939 - val_acc: 0.7889\n",
      "Epoch 52/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.7810 - acc: 0.7431 - val_loss: 0.5711 - val_acc: 0.8199\n",
      "Epoch 53/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.7859 - acc: 0.7428 - val_loss: 0.6696 - val_acc: 0.7863\n",
      "Epoch 54/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.7799 - acc: 0.7464 - val_loss: 0.6286 - val_acc: 0.8056\n",
      "Epoch 55/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.7623 - acc: 0.7483 - val_loss: 0.7640 - val_acc: 0.7825\n",
      "Epoch 56/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.7766 - acc: 0.7489 - val_loss: 0.6617 - val_acc: 0.8000\n",
      "Epoch 57/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.7555 - acc: 0.7521 - val_loss: 0.6039 - val_acc: 0.8133\n",
      "Epoch 58/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.7477 - acc: 0.7541 - val_loss: 0.5688 - val_acc: 0.8241\n",
      "Epoch 59/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.7649 - acc: 0.7493 - val_loss: 0.7108 - val_acc: 0.7754\n",
      "Epoch 60/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.7578 - acc: 0.7536 - val_loss: 0.8531 - val_acc: 0.7565\n",
      "Epoch 61/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.7449 - acc: 0.7562 - val_loss: 0.8234 - val_acc: 0.7618\n",
      "Epoch 62/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.7531 - acc: 0.7569 - val_loss: 0.6852 - val_acc: 0.7990\n",
      "Epoch 63/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.7523 - acc: 0.7544 - val_loss: 0.7854 - val_acc: 0.7792\n",
      "Epoch 64/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.7454 - acc: 0.7528 - val_loss: 0.6456 - val_acc: 0.8050\n",
      "Epoch 65/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.7363 - acc: 0.7638 - val_loss: 0.5217 - val_acc: 0.8350\n",
      "Epoch 66/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.7344 - acc: 0.7604 - val_loss: 0.6769 - val_acc: 0.7988\n",
      "Epoch 67/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.7341 - acc: 0.7624 - val_loss: 0.5469 - val_acc: 0.8187\n",
      "Epoch 68/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.7397 - acc: 0.7560 - val_loss: 0.5540 - val_acc: 0.8191\n",
      "Epoch 69/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.7242 - acc: 0.7649 - val_loss: 0.7580 - val_acc: 0.7785\n",
      "Epoch 70/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.7354 - acc: 0.7607 - val_loss: 0.6298 - val_acc: 0.8094\n",
      "Epoch 71/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.7248 - acc: 0.7633 - val_loss: 0.8028 - val_acc: 0.7754\n",
      "Epoch 72/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.7281 - acc: 0.7627 - val_loss: 0.6176 - val_acc: 0.8035\n",
      "Epoch 73/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.7334 - acc: 0.7581 - val_loss: 0.6164 - val_acc: 0.8110\n",
      "Epoch 74/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.7225 - acc: 0.7633 - val_loss: 0.5553 - val_acc: 0.8283\n",
      "Epoch 75/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.7249 - acc: 0.7644 - val_loss: 0.6309 - val_acc: 0.7995\n",
      "Epoch 76/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.7204 - acc: 0.7656 - val_loss: 0.7056 - val_acc: 0.7965\n",
      "Epoch 77/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.7297 - acc: 0.7616 - val_loss: 0.5386 - val_acc: 0.8292\n",
      "Epoch 78/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.7281 - acc: 0.7618 - val_loss: 0.5264 - val_acc: 0.8348\n",
      "Epoch 79/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.7268 - acc: 0.7620 - val_loss: 0.6621 - val_acc: 0.8079\n",
      "Epoch 80/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.7199 - acc: 0.7668 - val_loss: 0.8058 - val_acc: 0.7795\n",
      "Epoch 81/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.7193 - acc: 0.7657 - val_loss: 0.7785 - val_acc: 0.7868\n",
      "Epoch 82/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.7255 - acc: 0.7654 - val_loss: 0.5505 - val_acc: 0.8259\n",
      "Epoch 83/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.7102 - acc: 0.7670 - val_loss: 0.4889 - val_acc: 0.8399\n",
      "Epoch 84/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.7189 - acc: 0.7663 - val_loss: 0.5477 - val_acc: 0.8212\n",
      "Epoch 85/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.7086 - acc: 0.7683 - val_loss: 0.5214 - val_acc: 0.8369\n",
      "Epoch 86/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.7165 - acc: 0.7631 - val_loss: 0.4856 - val_acc: 0.8418\n",
      "Epoch 87/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.7195 - acc: 0.7638 - val_loss: 0.7300 - val_acc: 0.7962\n",
      "Epoch 88/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6962 - acc: 0.7722 - val_loss: 0.6030 - val_acc: 0.8194\n",
      "Epoch 89/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.7218 - acc: 0.7652 - val_loss: 0.6691 - val_acc: 0.8086\n",
      "Epoch 90/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.7017 - acc: 0.7702 - val_loss: 0.7072 - val_acc: 0.8017\n",
      "Epoch 91/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.7036 - acc: 0.7710 - val_loss: 0.5122 - val_acc: 0.8311\n",
      "Epoch 92/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.7045 - acc: 0.7728 - val_loss: 0.5340 - val_acc: 0.8343\n",
      "Epoch 93/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6980 - acc: 0.7718 - val_loss: 0.5888 - val_acc: 0.8174\n",
      "Epoch 94/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.7112 - acc: 0.7666 - val_loss: 0.5520 - val_acc: 0.8279\n",
      "Epoch 95/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.7063 - acc: 0.7697 - val_loss: 0.5247 - val_acc: 0.8381\n",
      "Epoch 96/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.7116 - acc: 0.7723 - val_loss: 0.6000 - val_acc: 0.8205\n",
      "Epoch 97/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6964 - acc: 0.7762 - val_loss: 0.7639 - val_acc: 0.7927\n",
      "Epoch 98/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.6996 - acc: 0.7727 - val_loss: 0.6523 - val_acc: 0.7961\n",
      "Epoch 99/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6888 - acc: 0.7753 - val_loss: 0.6633 - val_acc: 0.8075\n",
      "Epoch 100/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6841 - acc: 0.7798 - val_loss: 0.5673 - val_acc: 0.8278\n",
      "Epoch 101/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.7039 - acc: 0.7715 - val_loss: 0.5054 - val_acc: 0.8393\n",
      "Epoch 102/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6998 - acc: 0.7741 - val_loss: 0.6561 - val_acc: 0.8071\n",
      "Epoch 103/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6944 - acc: 0.7722 - val_loss: 0.5269 - val_acc: 0.8298\n",
      "Epoch 104/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6963 - acc: 0.7730 - val_loss: 0.6692 - val_acc: 0.8024\n",
      "Epoch 105/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6902 - acc: 0.7755 - val_loss: 0.5567 - val_acc: 0.8265\n",
      "Epoch 106/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6951 - acc: 0.7729 - val_loss: 0.5097 - val_acc: 0.8453\n",
      "Epoch 107/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6895 - acc: 0.7777 - val_loss: 0.5113 - val_acc: 0.8340\n",
      "Epoch 108/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6935 - acc: 0.7710 - val_loss: 0.5236 - val_acc: 0.8316\n",
      "Epoch 109/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6925 - acc: 0.7758 - val_loss: 0.7927 - val_acc: 0.7809\n",
      "Epoch 110/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6770 - acc: 0.7809 - val_loss: 0.5926 - val_acc: 0.8227\n",
      "Epoch 111/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6881 - acc: 0.7756 - val_loss: 0.6101 - val_acc: 0.8101\n",
      "Epoch 112/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6986 - acc: 0.7745 - val_loss: 0.5386 - val_acc: 0.8344\n",
      "Epoch 113/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.6790 - acc: 0.7811 - val_loss: 0.8420 - val_acc: 0.7791\n",
      "Epoch 114/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6842 - acc: 0.7764 - val_loss: 0.5625 - val_acc: 0.8219\n",
      "Epoch 115/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6796 - acc: 0.7761 - val_loss: 0.4688 - val_acc: 0.8517\n",
      "Epoch 116/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6871 - acc: 0.7770 - val_loss: 0.6816 - val_acc: 0.8049\n",
      "Epoch 117/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6919 - acc: 0.7786 - val_loss: 0.5572 - val_acc: 0.8309\n",
      "Epoch 118/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6740 - acc: 0.7822 - val_loss: 0.5753 - val_acc: 0.8171\n",
      "Epoch 119/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6781 - acc: 0.7843 - val_loss: 0.5171 - val_acc: 0.8392\n",
      "Epoch 120/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6758 - acc: 0.7787 - val_loss: 1.2253 - val_acc: 0.7269\n",
      "Epoch 121/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.6740 - acc: 0.7826 - val_loss: 0.7068 - val_acc: 0.7979\n",
      "Epoch 122/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.6814 - acc: 0.7810 - val_loss: 0.5209 - val_acc: 0.8401\n",
      "Epoch 123/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6862 - acc: 0.7750 - val_loss: 0.4897 - val_acc: 0.8439\n",
      "Epoch 124/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6772 - acc: 0.7771 - val_loss: 0.5954 - val_acc: 0.8127\n",
      "Epoch 125/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.6791 - acc: 0.7779 - val_loss: 0.5544 - val_acc: 0.8283\n",
      "Epoch 126/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6681 - acc: 0.7853 - val_loss: 0.5718 - val_acc: 0.8243\n",
      "Epoch 127/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6880 - acc: 0.7782 - val_loss: 0.5151 - val_acc: 0.8335\n",
      "Epoch 128/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6792 - acc: 0.7807 - val_loss: 0.5288 - val_acc: 0.8374\n",
      "Epoch 129/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6715 - acc: 0.7818 - val_loss: 0.4691 - val_acc: 0.8510\n",
      "Epoch 130/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6886 - acc: 0.7795 - val_loss: 0.6273 - val_acc: 0.8034\n",
      "Epoch 131/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.6784 - acc: 0.7807 - val_loss: 0.5239 - val_acc: 0.8336\n",
      "Epoch 132/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.6651 - acc: 0.7841 - val_loss: 0.4897 - val_acc: 0.8463\n",
      "Epoch 133/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6829 - acc: 0.7791 - val_loss: 0.5456 - val_acc: 0.8263\n",
      "Epoch 134/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.6697 - acc: 0.7835 - val_loss: 0.6978 - val_acc: 0.8037\n",
      "Epoch 135/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.6726 - acc: 0.7834 - val_loss: 0.4934 - val_acc: 0.8450\n",
      "Epoch 136/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6828 - acc: 0.7778 - val_loss: 0.6609 - val_acc: 0.8175\n",
      "Epoch 137/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.6664 - acc: 0.7845 - val_loss: 0.5538 - val_acc: 0.8311\n",
      "Epoch 138/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.6763 - acc: 0.7822 - val_loss: 0.5329 - val_acc: 0.8379\n",
      "Epoch 139/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.6682 - acc: 0.7829 - val_loss: 0.4895 - val_acc: 0.8427\n",
      "Epoch 140/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.6661 - acc: 0.7870 - val_loss: 0.5672 - val_acc: 0.8264\n",
      "Epoch 141/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.6786 - acc: 0.7791 - val_loss: 0.5885 - val_acc: 0.8310\n",
      "Epoch 142/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6720 - acc: 0.7861 - val_loss: 0.6096 - val_acc: 0.7928\n",
      "Epoch 143/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6668 - acc: 0.7833 - val_loss: 0.5858 - val_acc: 0.8265\n",
      "Epoch 144/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.6665 - acc: 0.7846 - val_loss: 0.5702 - val_acc: 0.8253\n",
      "Epoch 145/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.6848 - acc: 0.7783 - val_loss: 0.5380 - val_acc: 0.8375\n",
      "Epoch 146/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.6637 - acc: 0.7825 - val_loss: 0.5769 - val_acc: 0.8270\n",
      "Epoch 147/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.6680 - acc: 0.7834 - val_loss: 0.4469 - val_acc: 0.8590\n",
      "Epoch 148/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6584 - acc: 0.7815 - val_loss: 0.5062 - val_acc: 0.8411\n",
      "Epoch 149/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6810 - acc: 0.7796 - val_loss: 0.6106 - val_acc: 0.8236\n",
      "Epoch 150/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.6849 - acc: 0.7809 - val_loss: 0.5386 - val_acc: 0.8415\n",
      "Epoch 151/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.6566 - acc: 0.7864 - val_loss: 0.5727 - val_acc: 0.8266\n",
      "Epoch 152/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.6646 - acc: 0.7876 - val_loss: 0.5466 - val_acc: 0.8431\n",
      "Epoch 153/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.6684 - acc: 0.7839 - val_loss: 0.4966 - val_acc: 0.8421\n",
      "Epoch 154/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.6699 - acc: 0.7818 - val_loss: 0.5786 - val_acc: 0.8271\n",
      "Epoch 155/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6647 - acc: 0.7829 - val_loss: 0.6840 - val_acc: 0.8070\n",
      "Epoch 156/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6676 - acc: 0.7837 - val_loss: 0.5559 - val_acc: 0.8388\n",
      "Epoch 157/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.6581 - acc: 0.7864 - val_loss: 0.5130 - val_acc: 0.8377\n",
      "Epoch 158/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.6535 - acc: 0.7881 - val_loss: 0.5441 - val_acc: 0.8315\n",
      "Epoch 159/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6621 - acc: 0.7837 - val_loss: 0.5680 - val_acc: 0.8292\n",
      "Epoch 160/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6576 - acc: 0.7872 - val_loss: 0.6628 - val_acc: 0.8176\n",
      "Epoch 161/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6679 - acc: 0.7885 - val_loss: 0.5420 - val_acc: 0.8329\n",
      "Epoch 162/300\n",
      "1250/1250 [==============================] - 27s 22ms/step - loss: 0.6593 - acc: 0.7888 - val_loss: 0.5765 - val_acc: 0.8280\n",
      "Epoch 163/300\n",
      "1250/1250 [==============================] - 27s 22ms/step - loss: 0.6715 - acc: 0.7835 - val_loss: 0.6040 - val_acc: 0.8212\n",
      "Epoch 164/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6690 - acc: 0.7851 - val_loss: 0.4948 - val_acc: 0.8454\n",
      "Epoch 165/300\n",
      "1250/1250 [==============================] - 27s 22ms/step - loss: 0.6512 - acc: 0.7869 - val_loss: 0.4571 - val_acc: 0.8545\n",
      "Epoch 166/300\n",
      "1250/1250 [==============================] - 28s 22ms/step - loss: 0.6589 - acc: 0.7842 - val_loss: 0.5155 - val_acc: 0.8401\n",
      "Epoch 167/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6584 - acc: 0.7878 - val_loss: 0.4632 - val_acc: 0.8579\n",
      "Epoch 168/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6656 - acc: 0.7846 - val_loss: 0.5302 - val_acc: 0.8268\n",
      "Epoch 169/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6651 - acc: 0.7821 - val_loss: 0.6589 - val_acc: 0.8123\n",
      "Epoch 170/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6541 - acc: 0.7908 - val_loss: 0.5570 - val_acc: 0.8378\n",
      "Epoch 171/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6630 - acc: 0.7873 - val_loss: 0.5223 - val_acc: 0.8348\n",
      "Epoch 172/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6624 - acc: 0.7887 - val_loss: 0.5358 - val_acc: 0.8324\n",
      "Epoch 173/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6524 - acc: 0.7895 - val_loss: 0.5006 - val_acc: 0.8453\n",
      "Epoch 174/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6616 - acc: 0.7837 - val_loss: 0.6647 - val_acc: 0.8146\n",
      "Epoch 175/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6555 - acc: 0.7881 - val_loss: 0.6186 - val_acc: 0.8065\n",
      "Epoch 176/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6654 - acc: 0.7830 - val_loss: 0.5152 - val_acc: 0.8468\n",
      "Epoch 177/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6593 - acc: 0.7921 - val_loss: 0.5613 - val_acc: 0.8241\n",
      "Epoch 178/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6641 - acc: 0.7870 - val_loss: 0.5608 - val_acc: 0.8353\n",
      "Epoch 179/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6530 - acc: 0.7893 - val_loss: 0.6435 - val_acc: 0.8240\n",
      "Epoch 180/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6587 - acc: 0.7873 - val_loss: 0.5239 - val_acc: 0.8406\n",
      "Epoch 181/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6529 - acc: 0.7865 - val_loss: 0.4677 - val_acc: 0.8512\n",
      "Epoch 182/300\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 0.6507 - acc: 0.7904 - val_loss: 0.5322 - val_acc: 0.8338\n",
      "Epoch 183/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6486 - acc: 0.7912 - val_loss: 0.6438 - val_acc: 0.8132\n",
      "Epoch 184/300\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.6454 - acc: 0.7917 - val_loss: 0.5424 - val_acc: 0.8437\n",
      "Epoch 185/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6520 - acc: 0.7920 - val_loss: 0.5315 - val_acc: 0.8454\n",
      "Epoch 186/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6452 - acc: 0.7887 - val_loss: 0.5189 - val_acc: 0.8393\n",
      "Epoch 187/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6386 - acc: 0.7941 - val_loss: 0.4675 - val_acc: 0.8535\n",
      "Epoch 188/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6477 - acc: 0.7915 - val_loss: 0.6247 - val_acc: 0.8254\n",
      "Epoch 189/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6629 - acc: 0.7879 - val_loss: 0.5148 - val_acc: 0.8449\n",
      "Epoch 190/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6545 - acc: 0.7897 - val_loss: 0.4851 - val_acc: 0.8457\n",
      "Epoch 191/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6568 - acc: 0.7846 - val_loss: 0.5128 - val_acc: 0.8448\n",
      "Epoch 192/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6575 - acc: 0.7924 - val_loss: 0.5421 - val_acc: 0.8407\n",
      "Epoch 193/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6563 - acc: 0.7892 - val_loss: 0.5110 - val_acc: 0.8377\n",
      "Epoch 194/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6404 - acc: 0.7941 - val_loss: 0.5562 - val_acc: 0.8307\n",
      "Epoch 195/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6635 - acc: 0.7869 - val_loss: 0.4935 - val_acc: 0.8516\n",
      "Epoch 196/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6589 - acc: 0.7885 - val_loss: 0.7245 - val_acc: 0.8191\n",
      "Epoch 197/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6602 - acc: 0.7896 - val_loss: 0.5785 - val_acc: 0.8311\n",
      "Epoch 198/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6571 - acc: 0.7880 - val_loss: 0.6304 - val_acc: 0.8208\n",
      "Epoch 199/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6636 - acc: 0.7886 - val_loss: 0.5901 - val_acc: 0.8252\n",
      "Epoch 200/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6643 - acc: 0.7890 - val_loss: 0.6844 - val_acc: 0.8073\n",
      "Epoch 201/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6559 - acc: 0.7867 - val_loss: 0.5402 - val_acc: 0.8334\n",
      "Epoch 202/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6545 - acc: 0.7891 - val_loss: 0.6770 - val_acc: 0.8012\n",
      "Epoch 203/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6466 - acc: 0.7913 - val_loss: 0.6116 - val_acc: 0.8229\n",
      "Epoch 204/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6628 - acc: 0.7889 - val_loss: 0.4926 - val_acc: 0.8437\n",
      "Epoch 205/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6484 - acc: 0.7900 - val_loss: 0.7363 - val_acc: 0.8060\n",
      "Epoch 206/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6558 - acc: 0.7890 - val_loss: 0.6676 - val_acc: 0.8203\n",
      "Epoch 207/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6502 - acc: 0.7934 - val_loss: 0.5604 - val_acc: 0.8208\n",
      "Epoch 208/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6489 - acc: 0.7924 - val_loss: 0.4428 - val_acc: 0.8562\n",
      "Epoch 209/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6577 - acc: 0.7885 - val_loss: 0.5162 - val_acc: 0.8415\n",
      "Epoch 210/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6519 - acc: 0.7905 - val_loss: 0.5444 - val_acc: 0.8335\n",
      "Epoch 211/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6670 - acc: 0.7838 - val_loss: 0.4911 - val_acc: 0.8477\n",
      "Epoch 212/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6596 - acc: 0.7894 - val_loss: 0.5042 - val_acc: 0.8463\n",
      "Epoch 213/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6527 - acc: 0.7904 - val_loss: 0.5144 - val_acc: 0.8409\n",
      "Epoch 214/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6470 - acc: 0.7911 - val_loss: 0.4886 - val_acc: 0.8519\n",
      "Epoch 215/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6659 - acc: 0.7851 - val_loss: 0.5613 - val_acc: 0.8307\n",
      "Epoch 216/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6519 - acc: 0.7861 - val_loss: 0.5486 - val_acc: 0.8326\n",
      "Epoch 217/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6587 - acc: 0.7867 - val_loss: 0.5421 - val_acc: 0.8381\n",
      "Epoch 218/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6414 - acc: 0.7913 - val_loss: 0.5170 - val_acc: 0.8418\n",
      "Epoch 219/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6557 - acc: 0.7885 - val_loss: 0.5186 - val_acc: 0.8419\n",
      "Epoch 220/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6513 - acc: 0.7924 - val_loss: 0.5571 - val_acc: 0.8361\n",
      "Epoch 221/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6541 - acc: 0.7864 - val_loss: 0.6544 - val_acc: 0.8133\n",
      "Epoch 222/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6589 - acc: 0.7874 - val_loss: 0.5684 - val_acc: 0.8300\n",
      "Epoch 223/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6499 - acc: 0.7912 - val_loss: 0.4726 - val_acc: 0.8476\n",
      "Epoch 224/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6452 - acc: 0.7880 - val_loss: 0.6421 - val_acc: 0.8086\n",
      "Epoch 225/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6516 - acc: 0.7896 - val_loss: 0.6235 - val_acc: 0.8265\n",
      "Epoch 226/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6634 - acc: 0.7890 - val_loss: 0.7191 - val_acc: 0.8095\n",
      "Epoch 227/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6444 - acc: 0.7950 - val_loss: 0.4433 - val_acc: 0.8573\n",
      "Epoch 228/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6523 - acc: 0.7890 - val_loss: 0.6972 - val_acc: 0.8104\n",
      "Epoch 229/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6582 - acc: 0.7880 - val_loss: 0.5138 - val_acc: 0.8463\n",
      "Epoch 230/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6500 - acc: 0.7884 - val_loss: 0.4708 - val_acc: 0.8528\n",
      "Epoch 231/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6487 - acc: 0.7913 - val_loss: 0.6671 - val_acc: 0.8162\n",
      "Epoch 232/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6510 - acc: 0.7924 - val_loss: 0.4627 - val_acc: 0.8533\n",
      "Epoch 233/300\n",
      "1250/1250 [==============================] - 27s 22ms/step - loss: 0.6563 - acc: 0.7910 - val_loss: 0.5209 - val_acc: 0.8410\n",
      "Epoch 234/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6513 - acc: 0.7918 - val_loss: 0.5780 - val_acc: 0.8312\n",
      "Epoch 235/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6624 - acc: 0.7883 - val_loss: 0.4871 - val_acc: 0.8549\n",
      "Epoch 236/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6410 - acc: 0.7929 - val_loss: 0.5929 - val_acc: 0.8259\n",
      "Epoch 237/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6478 - acc: 0.7950 - val_loss: 0.4874 - val_acc: 0.8496\n",
      "Epoch 238/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6379 - acc: 0.7963 - val_loss: 0.4719 - val_acc: 0.8567\n",
      "Epoch 239/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6437 - acc: 0.7923 - val_loss: 0.5225 - val_acc: 0.8434\n",
      "Epoch 240/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6516 - acc: 0.7887 - val_loss: 0.4505 - val_acc: 0.8622\n",
      "Epoch 241/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6464 - acc: 0.7941 - val_loss: 0.4664 - val_acc: 0.8536\n",
      "Epoch 242/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6662 - acc: 0.7867 - val_loss: 0.4997 - val_acc: 0.8470\n",
      "Epoch 243/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6476 - acc: 0.7938 - val_loss: 0.4531 - val_acc: 0.8584\n",
      "Epoch 244/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6421 - acc: 0.7951 - val_loss: 0.5478 - val_acc: 0.8354\n",
      "Epoch 245/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6545 - acc: 0.7890 - val_loss: 0.5248 - val_acc: 0.8361\n",
      "Epoch 246/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6366 - acc: 0.7975 - val_loss: 0.5304 - val_acc: 0.8437\n",
      "Epoch 247/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6433 - acc: 0.7943 - val_loss: 0.4767 - val_acc: 0.8537\n",
      "Epoch 248/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6457 - acc: 0.7903 - val_loss: 0.5037 - val_acc: 0.8451\n",
      "Epoch 249/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6446 - acc: 0.7937 - val_loss: 0.4716 - val_acc: 0.8577\n",
      "Epoch 250/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6533 - acc: 0.7925 - val_loss: 0.4890 - val_acc: 0.8561\n",
      "Epoch 251/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6459 - acc: 0.7929 - val_loss: 0.5384 - val_acc: 0.8418\n",
      "Epoch 252/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6548 - acc: 0.7916 - val_loss: 0.5677 - val_acc: 0.8314\n",
      "Epoch 253/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6537 - acc: 0.7910 - val_loss: 0.4452 - val_acc: 0.8572\n",
      "Epoch 254/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6512 - acc: 0.7921 - val_loss: 0.6099 - val_acc: 0.8377\n",
      "Epoch 255/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6438 - acc: 0.7937 - val_loss: 0.5188 - val_acc: 0.8476\n",
      "Epoch 256/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6351 - acc: 0.7959 - val_loss: 0.5682 - val_acc: 0.8339\n",
      "Epoch 257/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6343 - acc: 0.7998 - val_loss: 0.4985 - val_acc: 0.8433\n",
      "Epoch 258/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6420 - acc: 0.7916 - val_loss: 0.4996 - val_acc: 0.8465\n",
      "Epoch 259/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6434 - acc: 0.7942 - val_loss: 0.4818 - val_acc: 0.8514\n",
      "Epoch 260/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6383 - acc: 0.7963 - val_loss: 0.4978 - val_acc: 0.8467\n",
      "Epoch 261/300\n",
      "1250/1250 [==============================] - 27s 22ms/step - loss: 0.6430 - acc: 0.7940 - val_loss: 0.5083 - val_acc: 0.8392\n",
      "Epoch 262/300\n",
      "1250/1250 [==============================] - 27s 22ms/step - loss: 0.6453 - acc: 0.7953 - val_loss: 0.4599 - val_acc: 0.8585\n",
      "Epoch 263/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6430 - acc: 0.7928 - val_loss: 0.4820 - val_acc: 0.8489\n",
      "Epoch 264/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6438 - acc: 0.7955 - val_loss: 0.5583 - val_acc: 0.8311\n",
      "Epoch 265/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6410 - acc: 0.7932 - val_loss: 0.4978 - val_acc: 0.8521\n",
      "Epoch 266/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6379 - acc: 0.7934 - val_loss: 0.4731 - val_acc: 0.8494\n",
      "Epoch 267/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6442 - acc: 0.7947 - val_loss: 0.4289 - val_acc: 0.8650\n",
      "Epoch 268/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6413 - acc: 0.7945 - val_loss: 0.4876 - val_acc: 0.8499\n",
      "Epoch 269/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6424 - acc: 0.7956 - val_loss: 0.4740 - val_acc: 0.8550\n",
      "Epoch 270/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6397 - acc: 0.7982 - val_loss: 0.5744 - val_acc: 0.8386\n",
      "Epoch 271/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6480 - acc: 0.7926 - val_loss: 0.4448 - val_acc: 0.8578\n",
      "Epoch 272/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6579 - acc: 0.7900 - val_loss: 0.4981 - val_acc: 0.8449\n",
      "Epoch 273/300\n",
      "1250/1250 [==============================] - 27s 22ms/step - loss: 0.6398 - acc: 0.7952 - val_loss: 0.5792 - val_acc: 0.8347\n",
      "Epoch 274/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6353 - acc: 0.7921 - val_loss: 0.5537 - val_acc: 0.8405\n",
      "Epoch 275/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6327 - acc: 0.7973 - val_loss: 0.8141 - val_acc: 0.7932\n",
      "Epoch 276/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6488 - acc: 0.7944 - val_loss: 0.4979 - val_acc: 0.8450\n",
      "Epoch 277/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6465 - acc: 0.7928 - val_loss: 0.5408 - val_acc: 0.8405\n",
      "Epoch 278/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6377 - acc: 0.7974 - val_loss: 0.5833 - val_acc: 0.8400\n",
      "Epoch 279/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6466 - acc: 0.7965 - val_loss: 0.5504 - val_acc: 0.8391\n",
      "Epoch 280/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6494 - acc: 0.7930 - val_loss: 0.4624 - val_acc: 0.8559\n",
      "Epoch 281/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6435 - acc: 0.7953 - val_loss: 0.5022 - val_acc: 0.8453\n",
      "Epoch 282/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6400 - acc: 0.7966 - val_loss: 0.5061 - val_acc: 0.8373\n",
      "Epoch 283/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6405 - acc: 0.7954 - val_loss: 0.5621 - val_acc: 0.8276\n",
      "Epoch 284/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6480 - acc: 0.7930 - val_loss: 0.5924 - val_acc: 0.8394\n",
      "Epoch 285/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6426 - acc: 0.7955 - val_loss: 0.4963 - val_acc: 0.8505\n",
      "Epoch 286/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6348 - acc: 0.7972 - val_loss: 0.4454 - val_acc: 0.8551\n",
      "Epoch 287/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6453 - acc: 0.7924 - val_loss: 0.5218 - val_acc: 0.8411\n",
      "Epoch 288/300\n",
      "1250/1250 [==============================] - 27s 22ms/step - loss: 0.6458 - acc: 0.7965 - val_loss: 0.5300 - val_acc: 0.8412\n",
      "Epoch 289/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6349 - acc: 0.7949 - val_loss: 0.7192 - val_acc: 0.8075\n",
      "Epoch 290/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6377 - acc: 0.7940 - val_loss: 0.5283 - val_acc: 0.8394\n",
      "Epoch 291/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6390 - acc: 0.7999 - val_loss: 0.5935 - val_acc: 0.8328\n",
      "Epoch 292/300\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 0.6418 - acc: 0.7965 - val_loss: 0.5290 - val_acc: 0.8362\n",
      "Epoch 293/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6385 - acc: 0.7949 - val_loss: 0.5109 - val_acc: 0.8447\n",
      "Epoch 294/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6425 - acc: 0.7950 - val_loss: 0.5227 - val_acc: 0.8493\n",
      "Epoch 295/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6471 - acc: 0.7916 - val_loss: 0.5780 - val_acc: 0.8380\n",
      "Epoch 296/300\n",
      "1250/1250 [==============================] - 27s 22ms/step - loss: 0.6446 - acc: 0.7948 - val_loss: 0.4616 - val_acc: 0.8612\n",
      "Epoch 297/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6479 - acc: 0.7891 - val_loss: 0.4793 - val_acc: 0.8558\n",
      "Epoch 298/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6359 - acc: 0.7959 - val_loss: 0.4597 - val_acc: 0.8563\n",
      "Epoch 299/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6493 - acc: 0.7951 - val_loss: 0.5135 - val_acc: 0.8491\n",
      "Epoch 300/300\n",
      "1250/1250 [==============================] - 27s 21ms/step - loss: 0.6327 - acc: 0.7989 - val_loss: 0.4445 - val_acc: 0.8549\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "history_augmt = model.fit(datagen.flow(x_tr, y_tr, batch_size=32), steps_per_epoch=x_tr.shape[0] // 32, epochs=epochs, validation_data=(x_val, y_val), validation_batch_size=x_val.shape[0] // 32)\n",
    "model.save('use_data_augmt_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "yhSzPolMuCrj",
    "outputId": "4182f8fb-c74d-436f-e1e2-d3849c39e297"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history_augmt.history['acc']\n",
    "val_acc = history_augmt.history['val_acc']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHd_phCGuCrk"
   },
   "source": [
    "## 3. Train (again) and evaluate the model\n",
    "\n",
    "- To this end, you have found the \"best\" hyper-parameters. \n",
    "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
    "- Evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loGH0psIuCrk"
   },
   "source": [
    "### 3.1. Train the model on the entire training set\n",
    "\n",
    "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "A7eqrhwNuCrk"
   },
   "outputs": [],
   "source": [
    "# <Compile your model again (using the same hyper-parameters)>\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), kernel_initializer='he_uniform', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "from keras import optimizers\n",
    " # to be tuned!\n",
    "learning_rate = 1E-3\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              metrics=['acc'])\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MXvLYugDuCrk",
    "outputId": "e7afbdb0-c441-472b-8183-10c8e8376651"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1562/1562 [==============================] - 66s 21ms/step - loss: 1.9812 - acc: 0.3014\n",
      "Epoch 2/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 1.4552 - acc: 0.4778\n",
      "Epoch 3/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 1.3162 - acc: 0.5389\n",
      "Epoch 4/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 1.2386 - acc: 0.5750\n",
      "Epoch 5/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 1.1884 - acc: 0.5930\n",
      "Epoch 6/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 1.1666 - acc: 0.6087\n",
      "Epoch 7/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 1.1108 - acc: 0.6255\n",
      "Epoch 8/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 1.1077 - acc: 0.6320\n",
      "Epoch 9/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 1.0594 - acc: 0.6462\n",
      "Epoch 10/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 1.0568 - acc: 0.6456\n",
      "Epoch 11/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 1.0368 - acc: 0.6572\n",
      "Epoch 12/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 1.0221 - acc: 0.6622\n",
      "Epoch 13/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 1.0026 - acc: 0.6621\n",
      "Epoch 14/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.9717 - acc: 0.6803\n",
      "Epoch 15/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.9653 - acc: 0.6789\n",
      "Epoch 16/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.9490 - acc: 0.6874\n",
      "Epoch 17/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.9426 - acc: 0.6874\n",
      "Epoch 18/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.9360 - acc: 0.6897\n",
      "Epoch 19/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.9125 - acc: 0.6938\n",
      "Epoch 20/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.9052 - acc: 0.7005\n",
      "Epoch 21/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.8954 - acc: 0.7023\n",
      "Epoch 22/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.8831 - acc: 0.7078\n",
      "Epoch 23/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.8700 - acc: 0.7155\n",
      "Epoch 24/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.8585 - acc: 0.7164\n",
      "Epoch 25/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.8359 - acc: 0.7235\n",
      "Epoch 26/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.8494 - acc: 0.7222\n",
      "Epoch 27/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8395 - acc: 0.7269\n",
      "Epoch 28/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.8235 - acc: 0.7253\n",
      "Epoch 29/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.8220 - acc: 0.7292\n",
      "Epoch 30/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.8243 - acc: 0.7295\n",
      "Epoch 31/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.8094 - acc: 0.7342\n",
      "Epoch 32/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.8021 - acc: 0.7308\n",
      "Epoch 33/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.8082 - acc: 0.7346\n",
      "Epoch 34/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.8003 - acc: 0.7396\n",
      "Epoch 35/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7809 - acc: 0.7445\n",
      "Epoch 36/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7894 - acc: 0.7388\n",
      "Epoch 37/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7871 - acc: 0.7411\n",
      "Epoch 38/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7762 - acc: 0.7439\n",
      "Epoch 39/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7694 - acc: 0.7501\n",
      "Epoch 40/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7673 - acc: 0.7497\n",
      "Epoch 41/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7707 - acc: 0.7507\n",
      "Epoch 42/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7634 - acc: 0.7498\n",
      "Epoch 43/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7673 - acc: 0.7481\n",
      "Epoch 44/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7603 - acc: 0.7500\n",
      "Epoch 45/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7620 - acc: 0.7488\n",
      "Epoch 46/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7476 - acc: 0.7555\n",
      "Epoch 47/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7446 - acc: 0.7573\n",
      "Epoch 48/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7429 - acc: 0.7551\n",
      "Epoch 49/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7461 - acc: 0.7576\n",
      "Epoch 50/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7468 - acc: 0.7534\n",
      "Epoch 51/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7515 - acc: 0.7590\n",
      "Epoch 52/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7389 - acc: 0.7579\n",
      "Epoch 53/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7298 - acc: 0.7648\n",
      "Epoch 54/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7370 - acc: 0.7600\n",
      "Epoch 55/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7214 - acc: 0.7653\n",
      "Epoch 56/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7307 - acc: 0.7625\n",
      "Epoch 57/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7229 - acc: 0.7646\n",
      "Epoch 58/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7230 - acc: 0.7652\n",
      "Epoch 59/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7223 - acc: 0.7665\n",
      "Epoch 60/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7164 - acc: 0.7681\n",
      "Epoch 61/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7357 - acc: 0.7599\n",
      "Epoch 62/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7107 - acc: 0.7664\n",
      "Epoch 63/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7119 - acc: 0.7705\n",
      "Epoch 64/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7275 - acc: 0.7607\n",
      "Epoch 65/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7051 - acc: 0.7689\n",
      "Epoch 66/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7137 - acc: 0.7678\n",
      "Epoch 67/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7164 - acc: 0.7670\n",
      "Epoch 68/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6943 - acc: 0.7742\n",
      "Epoch 69/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7022 - acc: 0.7716\n",
      "Epoch 70/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7083 - acc: 0.7711\n",
      "Epoch 71/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7005 - acc: 0.7735\n",
      "Epoch 72/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6991 - acc: 0.7727\n",
      "Epoch 73/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7025 - acc: 0.7718\n",
      "Epoch 74/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7031 - acc: 0.7736\n",
      "Epoch 75/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7081 - acc: 0.7707\n",
      "Epoch 76/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6903 - acc: 0.7749\n",
      "Epoch 77/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6831 - acc: 0.7747\n",
      "Epoch 78/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.7061 - acc: 0.7720\n",
      "Epoch 79/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6980 - acc: 0.7720\n",
      "Epoch 80/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6906 - acc: 0.7753\n",
      "Epoch 81/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6965 - acc: 0.7722\n",
      "Epoch 82/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6979 - acc: 0.7750\n",
      "Epoch 83/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6973 - acc: 0.7759\n",
      "Epoch 84/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6907 - acc: 0.7779\n",
      "Epoch 85/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6856 - acc: 0.7800\n",
      "Epoch 86/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6913 - acc: 0.7743\n",
      "Epoch 87/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6933 - acc: 0.7751\n",
      "Epoch 88/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6861 - acc: 0.7782\n",
      "Epoch 89/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6946 - acc: 0.7751\n",
      "Epoch 90/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6767 - acc: 0.7795\n",
      "Epoch 91/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6964 - acc: 0.7755\n",
      "Epoch 92/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6776 - acc: 0.7820\n",
      "Epoch 93/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6804 - acc: 0.7802\n",
      "Epoch 94/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6943 - acc: 0.7768\n",
      "Epoch 95/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6736 - acc: 0.7832\n",
      "Epoch 96/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6865 - acc: 0.7782\n",
      "Epoch 97/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6825 - acc: 0.7804\n",
      "Epoch 98/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6852 - acc: 0.7769\n",
      "Epoch 99/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6800 - acc: 0.7809\n",
      "Epoch 100/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6779 - acc: 0.7812\n",
      "Epoch 101/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6786 - acc: 0.7778\n",
      "Epoch 102/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6798 - acc: 0.7805\n",
      "Epoch 103/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6752 - acc: 0.7829\n",
      "Epoch 104/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6718 - acc: 0.7809\n",
      "Epoch 105/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6767 - acc: 0.7842\n",
      "Epoch 106/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6669 - acc: 0.7858\n",
      "Epoch 107/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6820 - acc: 0.7809\n",
      "Epoch 108/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6754 - acc: 0.7796\n",
      "Epoch 109/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6680 - acc: 0.7845\n",
      "Epoch 110/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6711 - acc: 0.7828\n",
      "Epoch 111/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6733 - acc: 0.7826\n",
      "Epoch 112/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6806 - acc: 0.7797\n",
      "Epoch 113/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6693 - acc: 0.7863\n",
      "Epoch 114/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6636 - acc: 0.7841\n",
      "Epoch 115/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6745 - acc: 0.7831\n",
      "Epoch 116/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6604 - acc: 0.7869\n",
      "Epoch 117/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6739 - acc: 0.7818\n",
      "Epoch 118/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6658 - acc: 0.7812\n",
      "Epoch 119/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6724 - acc: 0.7847\n",
      "Epoch 120/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6704 - acc: 0.7837\n",
      "Epoch 121/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6726 - acc: 0.7880\n",
      "Epoch 122/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6539 - acc: 0.7882\n",
      "Epoch 123/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6564 - acc: 0.7886\n",
      "Epoch 124/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6685 - acc: 0.7838\n",
      "Epoch 125/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6670 - acc: 0.7848\n",
      "Epoch 126/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6714 - acc: 0.7847\n",
      "Epoch 127/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6651 - acc: 0.7879\n",
      "Epoch 128/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6749 - acc: 0.7828\n",
      "Epoch 129/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6680 - acc: 0.7848\n",
      "Epoch 130/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6651 - acc: 0.7853\n",
      "Epoch 131/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6681 - acc: 0.7857\n",
      "Epoch 132/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6696 - acc: 0.7820\n",
      "Epoch 133/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6682 - acc: 0.7830\n",
      "Epoch 134/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6634 - acc: 0.7849\n",
      "Epoch 135/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6678 - acc: 0.7848\n",
      "Epoch 136/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6662 - acc: 0.7824\n",
      "Epoch 137/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6543 - acc: 0.7883\n",
      "Epoch 138/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6603 - acc: 0.7872\n",
      "Epoch 139/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6589 - acc: 0.7878\n",
      "Epoch 140/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6525 - acc: 0.7897\n",
      "Epoch 141/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6558 - acc: 0.7902\n",
      "Epoch 142/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6682 - acc: 0.7862\n",
      "Epoch 143/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6703 - acc: 0.7816\n",
      "Epoch 144/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6654 - acc: 0.7859\n",
      "Epoch 145/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6536 - acc: 0.7901\n",
      "Epoch 146/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6684 - acc: 0.7808\n",
      "Epoch 147/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6581 - acc: 0.7893\n",
      "Epoch 148/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6616 - acc: 0.7879\n",
      "Epoch 149/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6619 - acc: 0.7880\n",
      "Epoch 150/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6623 - acc: 0.7871\n",
      "Epoch 151/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6454 - acc: 0.7902\n",
      "Epoch 152/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6447 - acc: 0.7947\n",
      "Epoch 153/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6550 - acc: 0.7893\n",
      "Epoch 154/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6588 - acc: 0.7896\n",
      "Epoch 155/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6455 - acc: 0.7928\n",
      "Epoch 156/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6484 - acc: 0.7932\n",
      "Epoch 157/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6530 - acc: 0.7910\n",
      "Epoch 158/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6531 - acc: 0.7917\n",
      "Epoch 159/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6550 - acc: 0.7895\n",
      "Epoch 160/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6441 - acc: 0.7919\n",
      "Epoch 161/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6478 - acc: 0.7932\n",
      "Epoch 162/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6595 - acc: 0.7890\n",
      "Epoch 163/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6550 - acc: 0.7908\n",
      "Epoch 164/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6625 - acc: 0.7879\n",
      "Epoch 165/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6572 - acc: 0.7908\n",
      "Epoch 166/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6465 - acc: 0.7944\n",
      "Epoch 167/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6533 - acc: 0.7914\n",
      "Epoch 168/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6473 - acc: 0.7902\n",
      "Epoch 169/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6538 - acc: 0.7910\n",
      "Epoch 170/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6547 - acc: 0.7894\n",
      "Epoch 171/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6543 - acc: 0.7914\n",
      "Epoch 172/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6545 - acc: 0.7908\n",
      "Epoch 173/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6572 - acc: 0.7892\n",
      "Epoch 174/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6524 - acc: 0.7925\n",
      "Epoch 175/300\n",
      "1562/1562 [==============================] - 31s 20ms/step - loss: 0.6571 - acc: 0.7903\n",
      "Epoch 176/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6589 - acc: 0.7911\n",
      "Epoch 177/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6522 - acc: 0.7895\n",
      "Epoch 178/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6542 - acc: 0.7900\n",
      "Epoch 179/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6597 - acc: 0.7872\n",
      "Epoch 180/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6532 - acc: 0.7921\n",
      "Epoch 181/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6542 - acc: 0.7917\n",
      "Epoch 182/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6505 - acc: 0.7905\n",
      "Epoch 183/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6560 - acc: 0.7903\n",
      "Epoch 184/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6503 - acc: 0.7907\n",
      "Epoch 185/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6566 - acc: 0.7912\n",
      "Epoch 186/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6507 - acc: 0.7937\n",
      "Epoch 187/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6561 - acc: 0.7895\n",
      "Epoch 188/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6535 - acc: 0.7919\n",
      "Epoch 189/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6517 - acc: 0.7902\n",
      "Epoch 190/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6496 - acc: 0.7933\n",
      "Epoch 191/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6450 - acc: 0.7959\n",
      "Epoch 192/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6513 - acc: 0.7927\n",
      "Epoch 193/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6631 - acc: 0.7869\n",
      "Epoch 194/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6556 - acc: 0.7901\n",
      "Epoch 195/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6464 - acc: 0.7922\n",
      "Epoch 196/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6547 - acc: 0.7906\n",
      "Epoch 197/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6576 - acc: 0.7899\n",
      "Epoch 198/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6467 - acc: 0.7948\n",
      "Epoch 199/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6541 - acc: 0.7924\n",
      "Epoch 200/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6542 - acc: 0.7888\n",
      "Epoch 201/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6499 - acc: 0.7932\n",
      "Epoch 202/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6506 - acc: 0.7935\n",
      "Epoch 203/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6594 - acc: 0.7898\n",
      "Epoch 204/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6535 - acc: 0.7907\n",
      "Epoch 205/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6492 - acc: 0.7915\n",
      "Epoch 206/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6420 - acc: 0.7962\n",
      "Epoch 207/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6483 - acc: 0.7940\n",
      "Epoch 208/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6527 - acc: 0.7920\n",
      "Epoch 209/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6560 - acc: 0.7911\n",
      "Epoch 210/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6444 - acc: 0.7913\n",
      "Epoch 211/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6650 - acc: 0.7855\n",
      "Epoch 212/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6501 - acc: 0.7926\n",
      "Epoch 213/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6575 - acc: 0.7924\n",
      "Epoch 214/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6501 - acc: 0.7921\n",
      "Epoch 215/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6484 - acc: 0.7944\n",
      "Epoch 216/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6518 - acc: 0.7917\n",
      "Epoch 217/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6462 - acc: 0.7958\n",
      "Epoch 218/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6457 - acc: 0.7936\n",
      "Epoch 219/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6547 - acc: 0.7913\n",
      "Epoch 220/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6468 - acc: 0.7931\n",
      "Epoch 221/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6614 - acc: 0.7877\n",
      "Epoch 222/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6451 - acc: 0.7959\n",
      "Epoch 223/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6529 - acc: 0.7902\n",
      "Epoch 224/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6505 - acc: 0.7938\n",
      "Epoch 225/300\n",
      "1562/1562 [==============================] - 33s 21ms/step - loss: 0.6543 - acc: 0.7909\n",
      "Epoch 226/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6500 - acc: 0.7934\n",
      "Epoch 227/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6440 - acc: 0.7970\n",
      "Epoch 228/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6445 - acc: 0.7939\n",
      "Epoch 229/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6558 - acc: 0.7902\n",
      "Epoch 230/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6494 - acc: 0.7918\n",
      "Epoch 231/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6531 - acc: 0.7931\n",
      "Epoch 232/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6516 - acc: 0.7892\n",
      "Epoch 233/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6489 - acc: 0.7924\n",
      "Epoch 234/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6536 - acc: 0.7867\n",
      "Epoch 235/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6430 - acc: 0.7969\n",
      "Epoch 236/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6598 - acc: 0.7886\n",
      "Epoch 237/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6499 - acc: 0.7950\n",
      "Epoch 238/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6390 - acc: 0.7944\n",
      "Epoch 239/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6373 - acc: 0.7975\n",
      "Epoch 240/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6387 - acc: 0.7992\n",
      "Epoch 241/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6457 - acc: 0.7910\n",
      "Epoch 242/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6671 - acc: 0.7879\n",
      "Epoch 243/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6538 - acc: 0.7935\n",
      "Epoch 244/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6463 - acc: 0.7957\n",
      "Epoch 245/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6474 - acc: 0.7937\n",
      "Epoch 246/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6438 - acc: 0.7957\n",
      "Epoch 247/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6426 - acc: 0.7965\n",
      "Epoch 248/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6534 - acc: 0.7913\n",
      "Epoch 249/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6420 - acc: 0.7943\n",
      "Epoch 250/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6505 - acc: 0.7928\n",
      "Epoch 251/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6491 - acc: 0.7963\n",
      "Epoch 252/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6514 - acc: 0.7916\n",
      "Epoch 253/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6396 - acc: 0.7947\n",
      "Epoch 254/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6497 - acc: 0.7927\n",
      "Epoch 255/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6425 - acc: 0.7945\n",
      "Epoch 256/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6534 - acc: 0.7949\n",
      "Epoch 257/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6401 - acc: 0.7954\n",
      "Epoch 258/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6425 - acc: 0.7943\n",
      "Epoch 259/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6486 - acc: 0.7928\n",
      "Epoch 260/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6497 - acc: 0.7951\n",
      "Epoch 261/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6366 - acc: 0.7972\n",
      "Epoch 262/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6441 - acc: 0.7974\n",
      "Epoch 263/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6478 - acc: 0.7939\n",
      "Epoch 264/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6525 - acc: 0.7925\n",
      "Epoch 265/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6467 - acc: 0.7956\n",
      "Epoch 266/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6366 - acc: 0.7943\n",
      "Epoch 267/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6451 - acc: 0.7969\n",
      "Epoch 268/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6426 - acc: 0.7960\n",
      "Epoch 269/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6403 - acc: 0.7967\n",
      "Epoch 270/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6392 - acc: 0.7962\n",
      "Epoch 271/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6481 - acc: 0.7939\n",
      "Epoch 272/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6413 - acc: 0.7941\n",
      "Epoch 273/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6488 - acc: 0.7932\n",
      "Epoch 274/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6398 - acc: 0.7968\n",
      "Epoch 275/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6599 - acc: 0.7912\n",
      "Epoch 276/300\n",
      "1562/1562 [==============================] - 33s 21ms/step - loss: 0.6483 - acc: 0.7938\n",
      "Epoch 277/300\n",
      "1562/1562 [==============================] - 33s 21ms/step - loss: 0.6441 - acc: 0.7944\n",
      "Epoch 278/300\n",
      "1562/1562 [==============================] - 33s 21ms/step - loss: 0.6425 - acc: 0.7947\n",
      "Epoch 279/300\n",
      "1562/1562 [==============================] - 33s 21ms/step - loss: 0.6482 - acc: 0.7931\n",
      "Epoch 280/300\n",
      "1562/1562 [==============================] - 33s 21ms/step - loss: 0.6508 - acc: 0.7949\n",
      "Epoch 281/300\n",
      "1562/1562 [==============================] - 33s 21ms/step - loss: 0.6405 - acc: 0.7949\n",
      "Epoch 282/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6429 - acc: 0.7962\n",
      "Epoch 283/300\n",
      "1562/1562 [==============================] - 33s 21ms/step - loss: 0.6412 - acc: 0.7989\n",
      "Epoch 284/300\n",
      "1562/1562 [==============================] - 33s 21ms/step - loss: 0.6406 - acc: 0.7934\n",
      "Epoch 285/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6476 - acc: 0.7939\n",
      "Epoch 286/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6493 - acc: 0.7942\n",
      "Epoch 287/300\n",
      "1562/1562 [==============================] - 33s 21ms/step - loss: 0.6522 - acc: 0.7936\n",
      "Epoch 288/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6450 - acc: 0.7951\n",
      "Epoch 289/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6416 - acc: 0.7974\n",
      "Epoch 290/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6579 - acc: 0.7946\n",
      "Epoch 291/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6359 - acc: 0.7975\n",
      "Epoch 292/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6436 - acc: 0.7965\n",
      "Epoch 293/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6392 - acc: 0.7958\n",
      "Epoch 294/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6530 - acc: 0.7930\n",
      "Epoch 295/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6532 - acc: 0.7914\n",
      "Epoch 296/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6516 - acc: 0.7932\n",
      "Epoch 297/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6448 - acc: 0.7955\n",
      "Epoch 298/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6507 - acc: 0.7937\n",
      "Epoch 299/300\n",
      "1562/1562 [==============================] - 32s 20ms/step - loss: 0.6464 - acc: 0.7935\n",
      "Epoch 300/300\n",
      "1562/1562 [==============================] - 32s 21ms/step - loss: 0.6401 - acc: 0.7964\n"
     ]
    }
   ],
   "source": [
    "#<Train your model on the entire training set (50K samples)>\n",
    "#<Use (x_train, y_train_vec) instead of (x_tr, y_tr)>\n",
    "#<Do NOT use the validation_data option (because now you do not have validation data)>\n",
    "\n",
    "epochs = 300\n",
    "# fits the model on batches with real-time data augmentation:\n",
    "history_final = model.fit(datagen.flow(x_train, y_train_vec, batch_size=32), steps_per_epoch=x_train.shape[0] // 32, epochs=epochs)\n",
    "model.save('final_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gic8GrSbuCrl"
   },
   "source": [
    "### 3.2. Evaluate the model on the test set\n",
    "\n",
    "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7-_cl9fFgddz",
    "outputId": "a74aff36-3e80-43db-ca93-16b72f59cda6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.4570 - acc: 0.8622\n",
      "loss = 0.4570103883743286\n",
      "accuracy = 0.8622000217437744\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('non_data_augmt_model.h5')\n",
    "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(loss_and_acc[0]))\n",
    "print('accuracy = ' + str(loss_and_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ail_3tK6gOhq",
    "outputId": "29f4bd4a-b0ca-4c4b-968e-4e004543fdfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 4ms/step - loss: 0.4563 - acc: 0.8533\n",
      "loss = 0.456317663192749\n",
      "accuracy = 0.8532999753952026\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('use_data_augmt_model.h5')\n",
    "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(loss_and_acc[0]))\n",
    "print('accuracy = ' + str(loss_and_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FefsKoFgf4vH",
    "outputId": "376d504a-7251-4b21-d4b9-10a430c8bf4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5047 - acc: 0.8469\n",
      "loss = 0.5046696066856384\n",
      "accuracy = 0.8468999862670898\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('final_model.h5')\n",
    "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(loss_and_acc[0]))\n",
    "print('accuracy = ' + str(loss_and_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqqWPn4PkbJO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "aC3U9A8fuCrb",
    "STEDr4e1uCrf",
    "0RdeBO6JuCrg"
   ],
   "name": "HM4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
